{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain is a metric used in decision trees to determine which feature provides the most useful split at each step. It measures how much uncertainty (called entropy) in the target variable is reduced after splitting the data based on a particular feature.\n",
        "\n",
        "A decision tree calculates the entropy of the parent node, then evaluates how the data is divided into child nodes for each feature. The difference between the parent entropy and the weighted entropy of the children is called Information Gain.\n",
        "\n",
        "The feature that results in the highest Information Gain is chosen for the split because it produces the purest and most informative partitions of the data. This process continues recursively, helping the decision tree build a structure that best separates the classes and improves prediction accuracy."
      ],
      "metadata": {
        "id": "QoSOvFhJQ32S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "1. Meaning\n",
        "\n",
        "Gini Impurity: Measures how often a sample would be misclassified if labels were\n",
        "assigned randomly according to class proportions.\n",
        "\n",
        "Entropy: Measures the level of uncertainty or randomness in the node.\n",
        "\n",
        "\n",
        "2. Strengths\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Faster to compute (no log function).\n",
        "\n",
        "Often produces slightly purer splits.\n",
        "\n",
        "Works well when classes are imbalanced.\n",
        "\n",
        "Entropy\n",
        "\n",
        "More theoretically grounded (Information Theory).\n",
        "\n",
        "More sensitive to probability changes when classes are evenly split (50–50).\n",
        "\n",
        "Good for maximizing Information Gain.\n",
        "\n",
        "3. Weaknesses\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Less interpretable from an information-theory perspective.\n",
        "\n",
        "May be biased toward features with many categories.\n",
        "\n",
        "Entropy\n",
        "\n",
        "Slightly slower due to logarithms.\n",
        "\n",
        "Can exaggerate uncertainty in near-balanced splits.\n",
        "\n",
        "4. When to Use Each (Use Cases)\n",
        "\n",
        "Use Gini Impurity when:\n",
        "\n",
        "You want faster training (default in CART/scikit-learn).\n",
        "\n",
        "Simplicity and performance matter more than theory.\n",
        "\n",
        "Use Entropy when:\n",
        "\n",
        "You want splits based strictly on Information Gain (ID3, C4.5).\n",
        "\n",
        "You want a purer theoretical understanding of node uncertainty."
      ],
      "metadata": {
        "id": "L38l_82USh2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning, also called early stopping, is a technique used to stop a decision tree from growing too deep and becoming overly complex.\n",
        "\n",
        " Instead of allowing the tree to fully grow and then trimming it, pre-pruning puts rules in place during tree construction to decide when to stop splitting a node.\n",
        "\n",
        "It prevents overfitting by stopping the algorithm early when further splits are unlikely to improve model performance."
      ],
      "metadata": {
        "id": "BfxL_PulSX56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "\n",
        "# Decision Tree Classifier using Gini Impurity\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (Iris dataset for demonstration)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Create and train the model using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Display results clearly\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7VohunpS0aO",
        "outputId": "ab59a5d5-69eb-4300-da58-82a9a83829ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression, but it is mainly popular for classification tasks.\n",
        "\n",
        "SVM works by finding the best possible boundary (called a hyperplane) that separates different classes in the data.\n",
        "\n",
        "It chooses this boundary in such a way that the margin — the distance between the hyperplane and the nearest data points — is as large as possible.\n",
        "\n",
        "These closest data points are called support vectors, and they determine the position of the boundary."
      ],
      "metadata": {
        "id": "wva0MpQhTBLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "The Kernel Trick is a smart shortcut used in SVM to deal with data that cannot be separated with a straight line.\n",
        "\n",
        "Imagine your data is mixed up in 2D and you cannot draw a line between the classes.\n",
        "But if you could lift the data into 3D, suddenly it becomes separable.\n",
        "\n",
        "Instead of actually increasing dimensions (which is slow and complicated), the kernel trick pretends to do this using a mathematical function.\n",
        "It lets SVM draw complex, curved boundaries without doing extra heavy calculations."
      ],
      "metadata": {
        "id": "Vwl7nGnETLr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and split data\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train both models\n",
        "linear_clf = SVC(kernel='linear').fit(X_train, y_train)\n",
        "rbf_clf = SVC(kernel='rbf').fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy scores\n",
        "print(\"Linear Kernel Accuracy:\", accuracy_score(y_test, linear_clf.predict(X_test)))\n",
        "print(\"RBF Kernel Accuracy   :\", accuracy_score(y_test, rbf_clf.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0KevcQ6Tb4K",
        "outputId": "b89714bb-09d9-407c-b662-0cc223f825a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy   : 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "The Naïve Bayes classifier is a probabilistic machine learning algorithm that uses Bayes’ Theorem to predict the class of a given data point.\n",
        "\n",
        "It is widely used in real-world applications like spam email detection, sentiment analysis, document classification, and recommendation systems because it is simple, fast, and effective, even with large datasets.\n",
        "\n",
        "Why is it called “Naïve”?\n",
        "\n",
        "It is called naïve because it makes a strong assumption that all features are independent of each other. In reality, features are often correlated, but Naïve Bayes ignores these relationships to simplify calculations."
      ],
      "metadata": {
        "id": "czezDU6WTuWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "1. Gaussian Naïve Bayes (GNB)\n",
        "\n",
        "Data Type: Continuous (numeric) data.\n",
        "\n",
        "Assumption: Features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Example Use Case: Predicting a person’s risk based on continuous measurements like height, weight, or blood pressure.\n",
        "\n",
        "How it works: Calculates probabilities using the Gaussian (bell-curve) formula.\n",
        "\n",
        "2. Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "Data Type: Discrete count data (frequencies).\n",
        "\n",
        "Assumption: Features represent counts of events (like word counts in text).\n",
        "\n",
        "Example Use Case: Text classification such as spam detection or document categorization.\n",
        "\n",
        "How it works: Calculates probabilities based on the frequency of each feature in each class.\n",
        "\n",
        "3. Bernoulli Naïve Bayes (BNB)\n",
        "\n",
        "Data Type: Binary features (0 or 1, yes/no, present/absent).\n",
        "\n",
        "Assumption: Features follow a Bernoulli distribution.\n",
        "\n",
        "Example Use Case: Text classification where we only care about whether a word appears or not, not how many times.\n",
        "\n",
        "How it works: Uses presence/absence of features to calculate probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZcBmPq6UGGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naive Bayes:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_h2VGi9UWNm",
        "outputId": "91afb32c-5c57-4450-eed2-77730a7ecc69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes: 0.9415\n"
          ]
        }
      ]
    }
  ]
}